{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Copy of ValueLabs2.ipynb","provenance":[{"file_id":"0Bx5N7qODdz0-Ny1oUzVrMGU0SlVGd1RDSV9NTVhCa0E4NlNr","timestamp":1573465963328}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"cells":[{"cell_type":"code","metadata":{"colab_type":"code","id":"hmagIsH7Nb2h","outputId":"96c223ea-e659-4490-cc10-2dc8e802f95e","executionInfo":{"status":"ok","timestamp":1573453203315,"user_tz":-330,"elapsed":14896,"user":{"displayName":"Bhanu sree Balisetty","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDP21Fovgv-54GdUi8eB52GjzNw-SFegRrRjvMChg=s64","userId":"06056630683889706511"}},"colab":{"base_uri":"https://localhost:8080/","height":309}},"source":["#import all the necessary packages.\n","from io import BytesIO\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","import warnings\n","from bs4 import BeautifulSoup\n","\n","import math\n","import time\n","import re\n","import os\n","from sklearn.metrics.pairwise import cosine_similarity  \n","from sklearn.metrics import pairwise_distances\n","from matplotlib import gridspec\n","from scipy.sparse import hstack\n","import plotly\n","import plotly.figure_factory as ff\n","from plotly.graph_objs import Scatter, Layout\n","\n","plotly.offline.init_notebook_mode(connected=True)\n","warnings.filterwarnings(\"ignore\")\n","\n","import gensim\n","import random\n","import pickle\n","from gensim.models import Word2Vec, KeyedVectors\n","# from sklearn.metrics.pairwise import cosine_similarity\n","# import nltk\n","# from nltk.corpus import stopwords\n","from nltk.corpus import stopwords\n","!pip install fuzzywuzzy\n","from fuzzywuzzy import fuzz\n","import nltk\n","nltk.download('stopwords')\n","!pip install distance\n","import distance"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"text/html":["        <script type=\"text/javascript\">\n","        window.PlotlyConfig = {MathJaxConfig: 'local'};\n","        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n","        if (typeof require !== 'undefined') {\n","        require.undef(\"plotly\");\n","        requirejs.config({\n","            paths: {\n","                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n","            }\n","        });\n","        require(['plotly'], function(Plotly) {\n","            window._Plotly = Plotly;\n","        });\n","        }\n","        </script>\n","        "]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Collecting fuzzywuzzy\n","  Downloading https://files.pythonhosted.org/packages/d8/f1/5a267addb30ab7eaa1beab2b9323073815da4551076554ecc890a3595ec9/fuzzywuzzy-0.17.0-py2.py3-none-any.whl\n","Installing collected packages: fuzzywuzzy\n","Successfully installed fuzzywuzzy-0.17.0\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","Collecting distance\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5c/1a/883e47df323437aefa0d0a92ccfb38895d9416bd0b56262c2e46a47767b8/Distance-0.1.3.tar.gz (180kB)\n","\u001b[K     |████████████████████████████████| 184kB 2.7MB/s \n","\u001b[?25hBuilding wheels for collected packages: distance\n","  Building wheel for distance (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for distance: filename=Distance-0.1.3-cp36-none-any.whl size=16261 sha256=318e22b325305655c2c3d5df498465e14bda67606c0d589c4b3746cf0a875737\n","  Stored in directory: /root/.cache/pip/wheels/d5/aa/e1/dbba9e7b6d397d645d0f12db1c66dbae9c5442b39b001db18e\n","Successfully built distance\n","Installing collected packages: distance\n","Successfully installed distance-0.1.3\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"e3WpUWnqNxAl","outputId":"29c30654-c565-402e-c425-cd97aefdd335","executionInfo":{"status":"ok","timestamp":1573453231479,"user_tz":-330,"elapsed":43044,"user":{"displayName":"Bhanu sree Balisetty","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDP21Fovgv-54GdUi8eB52GjzNw-SFegRrRjvMChg=s64","userId":"06056630683889706511"}},"colab":{"base_uri":"https://localhost:8080/","height":122}},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"E2vmb4UKNb2q","cellView":"code","outputId":"32d8fe44-c0a1-48d4-c2f2-47ece2c34857","executionInfo":{"status":"ok","timestamp":1573453232665,"user_tz":-330,"elapsed":44217,"user":{"displayName":"Bhanu sree Balisetty","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDP21Fovgv-54GdUi8eB52GjzNw-SFegRrRjvMChg=s64","userId":"06056630683889706511"}},"colab":{"base_uri":"https://localhost:8080/","height":289}},"source":["data = pd.read_csv('/content/gdrive/My Drive/Colab Notebooks/ValueLabs/Train.csv')\n","# data = pd.read_csv('Train.csv')\n","print(data.shape)\n","data.head()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["(31499, 3)\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>question</th>\n","      <th>answer_text</th>\n","      <th>distractor</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Meals can be served</td>\n","      <td>in rooms at 9:00 p. m.</td>\n","      <td>'outside the room at 3:00 p. m.', 'in the dini...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>It can be inferred from the passage that</td>\n","      <td>The local government can deal with the problem...</td>\n","      <td>'If some tragedies occur again ', ' relevant d...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>The author called Tommy 's parents in order to</td>\n","      <td>help them realize their influence on Tommy</td>\n","      <td>'blame Tommy for his failing grades', 'blame T...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>It can be inferred from the passage that</td>\n","      <td>the writer is not very willing to use idioms</td>\n","      <td>'idioms are the most important part in a langu...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>How can we deal with snake wounds according to...</td>\n","      <td>Stay calm and do n't move .</td>\n","      <td>'Cut the wound and suck the poison out .'</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                            question  ...                                         distractor\n","0                                Meals can be served  ...  'outside the room at 3:00 p. m.', 'in the dini...\n","1           It can be inferred from the passage that  ...  'If some tragedies occur again ', ' relevant d...\n","2     The author called Tommy 's parents in order to  ...  'blame Tommy for his failing grades', 'blame T...\n","3           It can be inferred from the passage that  ...  'idioms are the most important part in a langu...\n","4  How can we deal with snake wounds according to...  ...          'Cut the wound and suck the poison out .'\n","\n","[5 rows x 3 columns]"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"code","metadata":{"id":"ZmLxO6yJIR6g","colab_type":"code","outputId":"f666b5e8-d847-48ce-858d-06d015ad67ac","executionInfo":{"status":"ok","timestamp":1573453234661,"user_tz":-330,"elapsed":46198,"user":{"displayName":"Bhanu sree Balisetty","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDP21Fovgv-54GdUi8eB52GjzNw-SFegRrRjvMChg=s64","userId":"06056630683889706511"}},"colab":{"base_uri":"https://localhost:8080/","height":204}},"source":["data.distractor = [i.split(',') for i in data.distractor]\n","data = data.explode('distractor')\n","#converting to lower case\n","data = data.apply(lambda x: x.astype(str).str.lower())\n","import re\n","# replacing _' with '\n","data = data.apply(lambda x: x.astype(str).str.replace(r\" '\", r\"'\"))\n","# replacing ( n't) with (n't)\n","data = data.apply(lambda x: x.astype(str).str.replace(r\" n't\", r\"n't\"))\n","# replacing ' s with 's\n","data = data.apply(lambda x: x.astype(str).str.replace(r\" 's\", r\"'s\"))\n","# removing speacial charecters\n","data = data.apply(lambda x: x.astype(str).str.replace(r\"[^A-Za-z0-9\\s]+\", r\"\"))\n","data.head()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>question</th>\n","      <th>answer_text</th>\n","      <th>distractor</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>meals can be served</td>\n","      <td>in rooms at 900 p m</td>\n","      <td>outside the room at 300 p m</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>meals can be served</td>\n","      <td>in rooms at 900 p m</td>\n","      <td>in the dining  room at 600 p m</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>meals can be served</td>\n","      <td>in rooms at 900 p m</td>\n","      <td>in the dining  room from 730 a m to 915 p m</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>it can be inferred from the passage that</td>\n","      <td>the local government can deal with the problem...</td>\n","      <td>if some tragedies occur again</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>it can be inferred from the passage that</td>\n","      <td>the local government can deal with the problem...</td>\n","      <td>relevant departments of the state council sho...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                   question  ...                                         distractor\n","0                       meals can be served  ...                        outside the room at 300 p m\n","0                       meals can be served  ...                     in the dining  room at 600 p m\n","0                       meals can be served  ...        in the dining  room from 730 a m to 915 p m\n","1  it can be inferred from the passage that  ...                      if some tragedies occur again\n","1  it can be inferred from the passage that  ...   relevant departments of the state council sho...\n","\n","[5 rows x 3 columns]"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"code","metadata":{"id":"uGOC3UtUYFSM","colab_type":"code","outputId":"6bf473fc-f999-40ce-b9bb-c8d071323b92","executionInfo":{"status":"ok","timestamp":1573453234664,"user_tz":-330,"elapsed":46189,"user":{"displayName":"Bhanu sree Balisetty","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDP21Fovgv-54GdUi8eB52GjzNw-SFegRrRjvMChg=s64","userId":"06056630683889706511"}},"colab":{"base_uri":"https://localhost:8080/","height":119}},"source":["# Collecting all the question and option strings to create a seperate w2v model for only questions\n","questions = data.question.drop_duplicates()\n","answer_strings = data.answer_text.drop_duplicates()\n","distractor_strings = data.distractor.drop_duplicates()\n","options = pd.concat([answer_strings, distractor_strings])\n","questions = questions.apply(lambda x: x.split(\" \"))\n","options = options.apply(lambda x: x.split(\" \"))\n","options.head()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0                           [in, rooms, at, 900, p, m]\n","1    [the, local, government, can, deal, with, the,...\n","2    [help, them, realize, their, influence, on, to...\n","3    [the, writer, is, not, very, willing, to, use,...\n","4                      [stay, calm, and, dont, move, ]\n","dtype: object"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Eeslnw0CNb29","colab":{}},"source":["X_questions = questions.to_numpy()\n","X_options = options.to_numpy()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"AOUe822sZIYE","colab_type":"code","outputId":"67e73cb5-2ae1-48c3-e8e9-2f6504226a11","executionInfo":{"status":"ok","timestamp":1573453234671,"user_tz":-330,"elapsed":46179,"user":{"displayName":"Bhanu sree Balisetty","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDP21Fovgv-54GdUi8eB52GjzNw-SFegRrRjvMChg=s64","userId":"06056630683889706511"}},"colab":{"base_uri":"https://localhost:8080/","height":156}},"source":["X_options"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([list(['in', 'rooms', 'at', '900', 'p', 'm']),\n","       list(['the', 'local', 'government', 'can', 'deal', 'with', 'the', 'problem', 'of', 'lacking', 'money', 'by', 'some', 'means', '']),\n","       list(['help', 'them', 'realize', 'their', 'influence', 'on', 'tommy']),\n","       ..., list(['he', 'liked', 'go', 'play', 'with', 'herbs', '']),\n","       list(['he', 'studied', 'at', 'a', 'herbal', 'medicine', 'store']),\n","       list(['his', 'family', 'had', 'a', 'herbal', 'medicine', 'store'])],\n","      dtype=object)"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"nHxDR2_1Nb3B","scrolled":true,"outputId":"5bc75c40-adb8-4c7e-c944-bd5c936e0c2e","executionInfo":{"status":"ok","timestamp":1573453237479,"user_tz":-330,"elapsed":48973,"user":{"displayName":"Bhanu sree Balisetty","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDP21Fovgv-54GdUi8eB52GjzNw-SFegRrRjvMChg=s64","userId":"06056630683889706511"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["%%time\n","w2v_ques_model = Word2Vec(X_questions, size=100, workers=4)\n","w2v_ques_model.save('/content/gdrive/My Drive/Colab Notebooks/ValueLabs/ques_word2vec.model')\n","# w2v_ques_model.save('ques_word2vec.model')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["CPU times: user 2.68 s, sys: 22.2 ms, total: 2.71 s\n","Wall time: 2.49 s\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"pNFMj1CuYwSW","colab":{}},"source":["for index, i in enumerate(X_options):\n","    X_options[index] = list(filter(lambda a: a != '', i))\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"uqZbmfI_cnYm","colab_type":"code","colab":{}},"source":["# X_options = np.unique(X_options)\n","w2v_opt_model = Word2Vec(X_options, size=100, workers=4)\n","# w2v_opt_model.save('opt_word2vec.model')\n","w2v_opt_model.save('/content/gdrive/My Drive/Colab Notebooks/ValueLabs/opt_word2vec.model')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"5C-gRws0Yuhv","colab":{}},"source":["def get_word_vec(sentence, text_type):\n","    if text_type == 'question':\n","        model = w2v_ques_model\n","        vocab = list(w2v_ques_model.wv.vocab)\n","    else:\n","        model = w2v_opt_model\n","        vocab = list(w2v_opt_model.wv.vocab)\n","\n","    vec = []\n","    for i in sentence:\n","        if i in vocab:\n","            vec.append(model[i])\n","        else:\n","            vec.append(np.ones(shape=(100,)))\n","    if len(sentence)==0:\n","        vec.append(np.ones(shape=(100,)))\n","    return np.mean(vec, axis=0)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"lK4paRVO4xn5","outputId":"f748f8ac-5ded-4b6e-a947-0c5b649ff8dd","executionInfo":{"status":"ok","timestamp":1573453249311,"user_tz":-330,"elapsed":60782,"user":{"displayName":"Bhanu sree Balisetty","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDP21Fovgv-54GdUi8eB52GjzNw-SFegRrRjvMChg=s64","userId":"06056630683889706511"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["%%time\n","change = False\n","if change or not os.path.isfile('/content/gdrive/My Drive/Colab Notebooks/ValueLabs/X_ques_vec.pkl'):\n","    X_ques_vec = {' '.join(xi):get_word_vec(xi, 'question') for xi in X_questions}\n","    f = open(\"/content/gdrive/My Drive/Colab Notebooks/ValueLabs/X_ques_vec.pkl\",\"wb\")\n","    pickle.dump(X_ques_vec,f)\n","    f.close()\n","if change or not os.path.isfile('/content/gdrive/My Drive/Colab Notebooks/ValueLabs/X_opt_vec.pkl'):\n","    X_opt_vec = {' '.join(xi):get_word_vec(xi, 'option') for xi in X_options}\n","    f = open(\"/content/gdrive/My Drive/Colab Notebooks/ValueLabs/X_opt_vec.pkl\",\"wb\")\n","    pickle.dump(X_opt_vec,f)\n","    f.close()\n","\n","f = open(\"/content/gdrive/My Drive/Colab Notebooks/ValueLabs/X_ques_vec.pkl\",'rb')\n","X_ques_vec = pickle.load(f)\n","f.close()\n","\n","f = open(\"/content/gdrive/My Drive/Colab Notebooks/ValueLabs/X_opt_vec.pkl\",'rb')\n","X_opt_vec = pickle.load(f)\n","f.close()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["CPU times: user 456 ms, sys: 117 ms, total: 573 ms\n","Wall time: 4.31 s\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"9neKz08hFEvg","colab_type":"code","outputId":"3c1c1354-953b-4026-b5af-d7acdc8839d4","executionInfo":{"status":"ok","timestamp":1573453249315,"user_tz":-330,"elapsed":60774,"user":{"displayName":"Bhanu sree Balisetty","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDP21Fovgv-54GdUi8eB52GjzNw-SFegRrRjvMChg=s64","userId":"06056630683889706511"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# len(X_opt_vec)\n","X_options.shape"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(99388,)"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"jDoGZmEdxAG9","colab":{}},"source":["## Creating new dataset with negative values for non generators\n","# %%time\n","change = False\n","if change or not os.path.isfile('/content/gdrive/My Drive/Colab Notebooks/ValueLabs/data3.csv'):\n","    data2 = pd.DataFrame(columns=data.columns)\n","    for index, i in enumerate(data.values):\n","        question = i[0]\n","        ans = i[1]\n","        distractor = i[2]\n","        l = len(X_options)\n","        new_dis = ' '.join(X_options[random.randint(0, l)])\n","        data2 = data2.append(pd.Series([question, ans, new_dis], index=data2.columns ), ignore_index=True)\n","    data['target'] = 1\n","    data2['target'] = 0\n","    data3 = pd.concat([data,data2], axis=0)\n","    \n","    data3.to_csv('/content/gdrive/My Drive/Colab Notebooks/ValueLabs/data3.csv')\n","\n","data3 = pd.read_csv('/content/gdrive/My Drive/Colab Notebooks/ValueLabs/data3.csv')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"O9oRPBqYJvuF","colab_type":"code","colab":{}},"source":["# dropping unnecessary columns\n","data3.drop(data3.columns[data3.columns.str.contains('unnamed',case = False)],axis = 1, inplace = True)\n","# Dropping values with na\n","data3 = data3.dropna()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6T0Nmq_AJ_DJ","colab_type":"code","colab":{}},"source":["def normalized_word_Common_qd(row):\n","    wq = set(map(lambda word: word.lower().strip(), row['question'].split(\" \")))\n","    wd = set(map(lambda word: word.lower().strip(), row['distractor'].split(\" \")))    \n","    return 1.0 * len(wq & wd)\n","\n","def normalized_word_Common_ad(row):\n","    wa = set(map(lambda word: word.lower().strip(), row['answer_text'].split(\" \")))    \n","    wd = set(map(lambda word: word.lower().strip(), row['distractor'].split(\" \")))    \n","    return 1.0 * len(wa & wd)\n","\n","def normalized_word_Total_qd(row):\n","    wq = set(map(lambda word: word.lower().strip(), row['question'].split(\" \")))\n","    wd = set(map(lambda word: word.lower().strip(), row['distractor'].split(\" \")))    \n","    return 1.0 * (len(wq) + len(wd))\n","    \n","def normalized_word_Total_ad(row):\n","    wa = set(map(lambda word: word.lower().strip(), row['answer_text'].split(\" \")))\n","    wd = set(map(lambda word: word.lower().strip(), row['distractor'].split(\" \")))    \n","    return 1.0 * (len(wa) + len(wd))\n","\n","def normalized_word_share_qd(row):\n","    wq = set(map(lambda word: word.lower().strip(), row['question'].split(\" \")))\n","    wd = set(map(lambda word: word.lower().strip(), row['distractor'].split(\" \")))    \n","    return 1.0 * len(wq & wd)/(len(wq) + len(wd))\n","\n","def normalized_word_share_ad(row):\n","    wa = set(map(lambda word: word.lower().strip(), row['answer_text'].split(\" \")))\n","    wd = set(map(lambda word: word.lower().strip(), row['distractor'].split(\" \")))    \n","    return 1.0 * len(wa & wd)/(len(wa) + len(wd))\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"oY2eAXg_x04S","colab_type":"code","outputId":"bd7e5881-8007-4f10-9ff4-ef2812d88c56","executionInfo":{"status":"ok","timestamp":1573453252252,"user_tz":-330,"elapsed":63687,"user":{"displayName":"Bhanu sree Balisetty","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDP21Fovgv-54GdUi8eB52GjzNw-SFegRrRjvMChg=s64","userId":"06056630683889706511"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["%%time\n","\n","\n","if not os.path.isfile('/content/gdrive/My Drive/Colab Notebooks/ValueLabs/data3_BasicFeature.csv'):\n","    # string length of question, answer_text, distractor\n","    data3['qlen'] = data3['question'].str.len() \n","    data3['alen'] = data3['answer_text'].str.len() \n","    data3['dlen'] = data3['distractor'].str.len() \n","\n","    # length of sentance of question, answer, distractor\n","    data3['q_n_words'] = data3['question'].apply(lambda row: len(row.split(\" \")))\n","    data3['a_n_words'] = data3['answer_text'].apply(lambda row: len(row.split(\" \")))\n","    data3['d_n_words'] = data3['distractor'].apply(lambda row: len(row.split(\" \")))\n","\n","    # common word count of question and distractor\n","    \n","    data3['word_Common_qd'] = data3.apply(normalized_word_Common_qd, axis=1)\n","\n","    # common word count of answer_text and distractor\n","    \n","    data3['word_Common_ad'] = data3.apply(normalized_word_Common_ad, axis=1)\n","\n","    # Total word count in question and distractor\n","    \n","    data3['word_Total_qd'] = data3.apply(normalized_word_Total_qd, axis=1)\n","\n","    # Total word count in answer_text and distractor\n","    \n","    data3['word_Total_ad'] = data3.apply(normalized_word_Total_ad, axis=1)\n","\n","\n","    # word share of question and distractor: word share = (word_common)/(word_total)\n","    \n","    data3['word_share_qd'] = data3.apply(normalized_word_share_qd, axis=1)\n","\n","    # word share of answer_text and distractor: word share = (word_common)/(word_total)\n","\n","    data3['word_share_ad'] = data3.apply(normalized_word_share_ad, axis=1)\n","\n","    data3.to_csv(\"/content/gdrive/My Drive/Colab Notebooks/ValueLabs/data3_BasicFeature.csv\", index=False)\n","\n","data3 = pd.read_csv(\"/content/gdrive/My Drive/Colab Notebooks/ValueLabs/data3_BasicFeature.csv\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["CPU times: user 378 ms, sys: 33.1 ms, total: 411 ms\n","Wall time: 1.62 s\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"T3XuEGFT_-A5","colab_type":"code","colab":{}},"source":["\n","\n","SAFE_DIV = 0.0001 \n","\n","STOP_WORDS = stopwords.words(\"english\")\n","\n","def get_token_features(q1, q2):\n","    token_features = [0.0]*10\n","    \n","    # Converting the Sentence into Tokens: \n","    q1_tokens = q1.split()\n","    q2_tokens = q2.split()\n","\n","    if len(q1_tokens) == 0 or len(q2_tokens) == 0:\n","        return token_features\n","    # Get the non-stopwords in Questions\n","    q1_words = set([word for word in q1_tokens if word not in STOP_WORDS])\n","    q2_words = set([word for word in q2_tokens if word not in STOP_WORDS])\n","    \n","    #Get the stopwords in Questions\n","    q1_stops = set([word for word in q1_tokens if word in STOP_WORDS])\n","    q2_stops = set([word for word in q2_tokens if word in STOP_WORDS])\n","    \n","    # Get the common non-stopwords from Question pair\n","    common_word_count = len(q1_words.intersection(q2_words))\n","    \n","    # Get the common stopwords from Question pair\n","    common_stop_count = len(q1_stops.intersection(q2_stops))\n","    \n","    # Get the common Tokens from Question pair\n","    common_token_count = len(set(q1_tokens).intersection(set(q2_tokens)))\n","    \n","    \n","    token_features[0] = common_word_count / (min(len(q1_words), len(q2_words)) + SAFE_DIV)\n","    token_features[1] = common_word_count / (max(len(q1_words), len(q2_words)) + SAFE_DIV)\n","    token_features[2] = common_stop_count / (min(len(q1_stops), len(q2_stops)) + SAFE_DIV)\n","    token_features[3] = common_stop_count / (max(len(q1_stops), len(q2_stops)) + SAFE_DIV)\n","    token_features[4] = common_token_count / (min(len(q1_tokens), len(q2_tokens)) + SAFE_DIV)\n","    token_features[5] = common_token_count / (max(len(q1_tokens), len(q2_tokens)) + SAFE_DIV)\n","    \n","    # Last word of both question is same or not\n","    token_features[6] = int(q1_tokens[-1] == q2_tokens[-1])\n","    \n","    # First word of both question is same or not\n","    token_features[7] = int(q1_tokens[0] == q2_tokens[0])\n","    \n","    token_features[8] = abs(len(q1_tokens) - len(q2_tokens))\n","    \n","    #Average Token Length of both Questions\n","    token_features[9] = (len(q1_tokens) + len(q2_tokens))/2\n","    return token_features\n","\n","# get the Longest Common sub string\n","\n","def get_longest_substr_ratio(a, b):\n","    strs = list(distance.lcsubstrings(a, b))\n","    if len(strs) == 0:\n","        return 0\n","    else:\n","        return len(strs[0]) / (min(len(a), len(b)) + 1)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"L_iXm4ipfq2_","colab_type":"code","outputId":"0cf52ce1-6610-4b82-b0d5-24649fbe1fec","executionInfo":{"status":"ok","timestamp":1573453255250,"user_tz":-330,"elapsed":66672,"user":{"displayName":"Bhanu sree Balisetty","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDP21Fovgv-54GdUi8eB52GjzNw-SFegRrRjvMChg=s64","userId":"06056630683889706511"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["%%time\n","if not os.path.isfile(\"/content/gdrive/My Drive/Colab Notebooks/ValueLabs/data3_with_nlp.csv\"):\n","    print(\"token features...\")\n","\n","    # Merging Features with dataset - Token features for question and distractor \n","    token_features = data3.apply(lambda x: get_token_features(x[\"question\"], x[\"distractor\"]), axis=1)\n","\n","    data3[\"cwc_min_qd\"]       = list(map(lambda x: x[0], token_features))\n","    data3[\"cwc_max_qd\"]       = list(map(lambda x: x[1], token_features))\n","    data3[\"csc_min_qd\"]       = list(map(lambda x: x[2], token_features))\n","    data3[\"csc_max_qd\"]       = list(map(lambda x: x[3], token_features))\n","    data3[\"ctc_min_qd\"]       = list(map(lambda x: x[4], token_features))\n","    data3[\"ctc_max_qd\"]       = list(map(lambda x: x[5], token_features))\n","    data3[\"last_word_eq_qd\"]  = list(map(lambda x: x[6], token_features))\n","    data3[\"first_word_eq_qd\"] = list(map(lambda x: x[7], token_features))\n","    data3[\"abs_len_diff_qd\"]  = list(map(lambda x: x[8], token_features))\n","    data3[\"mean_len_qd\"]      = list(map(lambda x: x[9], token_features))\n","\n","    # Merging Features with dataset - Token features for answer_text and distractor \n","    token_features = data3.apply(lambda x: get_token_features(x[\"answer_text\"], x[\"distractor\"]), axis=1)\n","\n","    data3[\"cwc_min_ad\"]       = list(map(lambda x: x[0], token_features))\n","    data3[\"cwc_max_ad\"]       = list(map(lambda x: x[1], token_features))\n","    data3[\"csc_min_ad\"]       = list(map(lambda x: x[2], token_features))\n","    data3[\"csc_max_ad\"]       = list(map(lambda x: x[3], token_features))\n","    data3[\"ctc_min_ad\"]       = list(map(lambda x: x[4], token_features))\n","    data3[\"ctc_max_ad\"]       = list(map(lambda x: x[5], token_features))\n","    data3[\"last_word_eq_ad\"]  = list(map(lambda x: x[6], token_features))\n","    data3[\"first_word_eq_ad\"] = list(map(lambda x: x[7], token_features))\n","    data3[\"abs_len_diff_ad\"]  = list(map(lambda x: x[8], token_features))\n","    data3[\"mean_len_ad\"]      = list(map(lambda x: x[9], token_features))\n","\n","\n","\n","    print(\"fuzzy features..\")\n","    data3[\"token_set_ratio_qd\"]       = data3.apply(lambda x: fuzz.token_set_ratio(x[\"question\"], x[\"distractor\"]), axis=1)\n","    data3[\"token_set_ratio_ad\"]       = data3.apply(lambda x: fuzz.token_set_ratio(x[\"answer_text\"], x[\"distractor\"]), axis=1)\n","\n","    data3[\"token_sort_ratio_qd\"]      = data3.apply(lambda x: fuzz.token_sort_ratio(x[\"question\"], x[\"distractor\"]), axis=1)\n","    data3[\"token_sort_ratio_ad\"]      = data3.apply(lambda x: fuzz.token_sort_ratio(x[\"answer_text\"], x[\"distractor\"]), axis=1)\n","\n","    data3[\"fuzz_ratio_qd\"]            = data3.apply(lambda x: fuzz.QRatio(x[\"question\"], x[\"distractor\"]), axis=1)\n","    data3[\"fuzz_ratio_ad\"]            = data3.apply(lambda x: fuzz.QRatio(x[\"answer_text\"], x[\"distractor\"]), axis=1)\n","\n","    data3[\"fuzz_partial_ratio_qd\"]    = data3.apply(lambda x: fuzz.partial_ratio(x[\"question\"], x[\"distractor\"]), axis=1)\n","    data3[\"fuzz_partial_ratio_ad\"]    = data3.apply(lambda x: fuzz.partial_ratio(x[\"answer_text\"], x[\"distractor\"]), axis=1)\n","\n","    data3[\"longest_substr_ratio_qd\"]  = data3.apply(lambda x: get_longest_substr_ratio(x[\"question\"], x[\"distractor\"]), axis=1)\n","    data3[\"longest_substr_ratio_ad\"]  = data3.apply(lambda x: get_longest_substr_ratio(x[\"answer_text\"], x[\"distractor\"]), axis=1)\n","\n","    data3.to_csv(\"/content/gdrive/My Drive/Colab Notebooks/ValueLabs/data3_with_nlp.csv\", index=False)\n","\n","data3 = pd.read_csv(\"/content/gdrive/My Drive/Colab Notebooks/ValueLabs/data3_with_nlp.csv\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["CPU times: user 820 ms, sys: 66.3 ms, total: 886 ms\n","Wall time: 2.55 s\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"KaLwcBPNKnmf","colab_type":"code","outputId":"5d9e100d-02fc-4bba-8c89-06e0b0101034","executionInfo":{"status":"ok","timestamp":1573453288884,"user_tz":-330,"elapsed":11927,"user":{"displayName":"Bhanu sree Balisetty","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDP21Fovgv-54GdUi8eB52GjzNw-SFegRrRjvMChg=s64","userId":"06056630683889706511"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["%%time\n","if not os.path.isfile(\"/content/gdrive/My Drive/Colab Notebooks/ValueLabs/data3_final.csv\"):\n","\n","    from tqdm import tqdm\n","    q_vec = pd.DataFrame()\n","    a_vec = pd.DataFrame()\n","    d_vec = pd.DataFrame()\n","\n","    for i in tqdm(data3.values):\n","        q_vec = q_vec.append([X_ques_vec.get(i[0], np.zeros(100))], ignore_index=True)\n","        a_vec = a_vec.append( [X_opt_vec.get(i[1], np.zeros(100))], ignore_index=True)\n","        d_vec = d_vec.append([X_opt_vec.get(i[2], np.zeros(100))], ignore_index=True)\n","    data3 = pd.concat([data3, q_vec, a_vec, d_vec], axis = 1)\n","    print(data3.shape)\n","    data3.to_csv(\"/content/gdrive/My Drive/Colab Notebooks/ValueLabs/data3_final.csv\", index=False)\n","data3 = pd.read_csv(\"/content/gdrive/My Drive/Colab Notebooks/ValueLabs/data3_final.csv\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["CPU times: user 9.34 s, sys: 552 ms, total: 9.89 s\n","Wall time: 10.8 s\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"OmxkaSATO7AE","colab_type":"code","colab":{}},"source":["# data3 = pd.concat([data3, q_vec, a_vec, d_vec], axis = 1)\n","# print(data3.shape)\n","# data3.to_csv(\"/content/gdrive/My Drive/Colab Notebooks/ValueLabs/data3_final.csv\", index=False)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6MrYbYPU6qF8","colab_type":"code","colab":{}},"source":["data3.columns"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Wl2VFOf8_T03","colab_type":"text"},"source":["# Training ML Model"]},{"cell_type":"code","metadata":{"id":"PbQ0V7P0_S64","colab_type":"code","colab":{}},"source":["from xgboost import XGBClassifier\n","clf = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n","              colsample_bynode=1, colsample_bytree=0.3, gamma=0,\n","              learning_rate=0.9, max_delta_step=0, max_depth=5,\n","              min_child_weight=1, missing=None, n_estimators=500, n_jobs=1,\n","              nthread=None, objective='binary:logistic', random_state=0,\n","              reg_alpha=0, reg_lambda=1, seed=None,\n","              silent=None, subsample=1, verbosity=1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9ZG0Vrm7_N5E","colab_type":"code","outputId":"0edda755-ac06-4f64-cab4-7379038b17ce","executionInfo":{"status":"ok","timestamp":1573453457394,"user_tz":-330,"elapsed":1526,"user":{"displayName":"Bhanu sree Balisetty","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDP21Fovgv-54GdUi8eB52GjzNw-SFegRrRjvMChg=s64","userId":"06056630683889706511"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["%%time\n","# X_train = data3.drop(['question', 'answer_text', 'distractor', 'target'], axis=1).to_numpy()\n","# y_train = data3.target\n","# model = clf.fit(X_train, y_train)\n","# # pickle.dump(model, open(\"/content/gdrive/My Drive/Colab Notebooks/ValueLabs/final_model.model\", \"wb\"))\n","model = pickle.load(open(\"/content/gdrive/My Drive/Colab Notebooks/ValueLabs/final_model.model\", \"rb\"))\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["CPU times: user 99 µs, sys: 3.92 ms, total: 4.02 ms\n","Wall time: 775 ms\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"jb5JhvqcBpnb","colab_type":"text"},"source":["# Applying model on Test data"]},{"cell_type":"code","metadata":{"id":"1mbL0a7QBw4w","colab_type":"code","colab":{}},"source":["def predict_distractor(df):\n","    #preprocessing\n","    #converting to lower case\n","    df = df.apply(lambda x: x.astype(str).str.lower())\n","    import re\n","    # replacing _' with '\n","    df = df.apply(lambda x: x.astype(str).str.replace(r\" '\", r\"'\"))\n","    # replacing ( n't) with (n't)\n","    df = df.apply(lambda x: x.astype(str).str.replace(r\" n't\", r\"n't\"))\n","    # replacing ' s with 's\n","    df = df.apply(lambda x: x.astype(str).str.replace(r\" 's\", r\"'s\"))\n","    # removing speacial charecters\n","    df = df.apply(lambda x: x.astype(str).str.replace(r\"[^A-Za-z0-9\\s]+\", r\"\"))\n","\n","    #Applying all possible distractors\n","    n = X_options.shape[0]\n","    df = pd.concat([df]*n, ignore_index=True)\n","    df['distractor'] = pd.Series(X_options).apply(lambda row: ' '.join(row))\n","    print(df.values[100])\n","    # basic features\n","    # string length of question, answer_text, distractor\n","    df['qlen'] = df['question'].str.len() \n","    df['alen'] = df['answer_text'].str.len() \n","    df['dlen'] = df['distractor'].str.len() \n","    # length of sentance of question, answer, distractor\n","    df['q_n_words'] = df['question'].apply(lambda row: len(row.split(\" \")))\n","    df['a_n_words'] = df['answer_text'].apply(lambda row: len(row.split(\" \")))\n","    df['d_n_words'] = df['distractor'].apply(lambda row: len(row))\n","    # common word count of question and distractor\n","    df['word_Common_qd'] = df.apply(normalized_word_Common_qd, axis=1)\n","    # common word count of answer_text and distractor\n","    df['word_Common_ad'] = df.apply(normalized_word_Common_ad, axis=1)\n","    # Total word count in question and distractor\n","    df['word_Total_qd'] = df.apply(normalized_word_Total_qd, axis=1)\n","    # Total word count in answer_text and distractor\n","    df['word_Total_ad'] = df.apply(normalized_word_Total_ad, axis=1)\n","    # word share of question and distractor: word share = (word_common)/(word_total)\n","    df['word_share_qd'] = df.apply(normalized_word_share_qd, axis=1)\n","    # word share of answer_text and distractor: word share = (word_common)/(word_total)\n","    df['word_share_ad'] = df.apply(normalized_word_share_ad, axis=1)\n","\n","    # nlp features\n","    token_features = df.apply(lambda x: get_token_features(x[\"question\"], x[\"distractor\"]), axis=1)\n","    df[\"cwc_min_qd\"]       = list(map(lambda x: x[0], token_features))\n","    df[\"cwc_max_qd\"]       = list(map(lambda x: x[1], token_features))\n","    df[\"csc_min_qd\"]       = list(map(lambda x: x[2], token_features))\n","    df[\"csc_max_qd\"]       = list(map(lambda x: x[3], token_features))\n","    df[\"ctc_min_qd\"]       = list(map(lambda x: x[4], token_features))\n","    df[\"ctc_max_qd\"]       = list(map(lambda x: x[5], token_features))\n","    df[\"last_word_eq_qd\"]  = list(map(lambda x: x[6], token_features))\n","    df[\"first_word_eq_qd\"] = list(map(lambda x: x[7], token_features))\n","    df[\"abs_len_diff_qd\"]  = list(map(lambda x: x[8], token_features))\n","    df[\"mean_len_qd\"]      = list(map(lambda x: x[9], token_features))\n","    # Merging Features with dataset - Token features for answer_text and distractor \n","    token_features = df.apply(lambda x: get_token_features(x[\"answer_text\"], x[\"distractor\"]), axis=1)\n","    df[\"cwc_min_ad\"]       = list(map(lambda x: x[0], token_features))\n","    df[\"cwc_max_ad\"]       = list(map(lambda x: x[1], token_features))\n","    df[\"csc_min_ad\"]       = list(map(lambda x: x[2], token_features))\n","    df[\"csc_max_ad\"]       = list(map(lambda x: x[3], token_features))\n","    df[\"ctc_min_ad\"]       = list(map(lambda x: x[4], token_features))\n","    df[\"ctc_max_ad\"]       = list(map(lambda x: x[5], token_features))\n","    df[\"last_word_eq_ad\"]  = list(map(lambda x: x[6], token_features))\n","    df[\"first_word_eq_ad\"] = list(map(lambda x: x[7], token_features))\n","    df[\"abs_len_diff_ad\"]  = list(map(lambda x: x[8], token_features))\n","    df[\"mean_len_ad\"]      = list(map(lambda x: x[9], token_features))\n","    print(\"fuzzy features..\")\n","    df[\"token_set_ratio_qd\"]       = df.apply(lambda x: fuzz.token_set_ratio(x[\"question\"], x[\"distractor\"]), axis=1)\n","    df[\"token_set_ratio_ad\"]       = df.apply(lambda x: fuzz.token_set_ratio(x[\"answer_text\"], x[\"distractor\"]), axis=1)\n","    df[\"token_sort_ratio_qd\"]      = df.apply(lambda x: fuzz.token_sort_ratio(x[\"question\"], x[\"distractor\"]), axis=1)\n","    df[\"token_sort_ratio_ad\"]      = df.apply(lambda x: fuzz.token_sort_ratio(x[\"answer_text\"], x[\"distractor\"]), axis=1)\n","    df[\"fuzz_ratio_qd\"]            = df.apply(lambda x: fuzz.QRatio(x[\"question\"], x[\"distractor\"]), axis=1)\n","    df[\"fuzz_ratio_ad\"]            = df.apply(lambda x: fuzz.QRatio(x[\"answer_text\"], x[\"distractor\"]), axis=1)\n","    df[\"fuzz_partial_ratio_qd\"]    = df.apply(lambda x: fuzz.partial_ratio(x[\"question\"], x[\"distractor\"]), axis=1)\n","    df[\"fuzz_partial_ratio_ad\"]    = df.apply(lambda x: fuzz.partial_ratio(x[\"answer_text\"], x[\"distractor\"]), axis=1)\n","    df[\"longest_substr_ratio_qd\"]  = df.apply(lambda x: get_longest_substr_ratio(x[\"question\"], x[\"distractor\"]), axis=1)\n","    df[\"longest_substr_ratio_ad\"]  = df.apply(lambda x: get_longest_substr_ratio(x[\"answer_text\"], x[\"distractor\"]), axis=1)\n","    print(df.shape)\n","    q_vec = pd.DataFrame()\n","    a_vec = pd.DataFrame()\n","    d_vec = pd.DataFrame()\n","    ques_vec = [get_word_vec(df.values[0][0], 'question')]\n","    ans_vec = [get_word_vec(df.values[0][1], 'option')]\n","    for i in df.values:\n","        q_vec = q_vec.append(ques_vec, ignore_index=True)\n","        a_vec = a_vec.append(ans_vec, ignore_index=True)\n","        d_vec = d_vec.append([X_opt_vec.get(i[2], np.zeros(100))], ignore_index=True)\n","    \n","    df = pd.concat([df, q_vec, a_vec, d_vec], axis = 1)\n","    print(df.shape)\n","    X_test = (df.drop(['question', 'answer_text', 'distractor'], axis=1)).to_numpy()\n","    df['target'] = model.predict(X_test)\n","    return df[df.target==1]\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"YeCngOWkEdIe","colab_type":"code","outputId":"7fcced27-1345-43c2-ec75-528865ee47d4","executionInfo":{"status":"ok","timestamp":1573462356586,"user_tz":-330,"elapsed":621909,"user":{"displayName":"Bhanu sree Balisetty","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDP21Fovgv-54GdUi8eB52GjzNw-SFegRrRjvMChg=s64","userId":"06056630683889706511"}},"colab":{"base_uri":"https://localhost:8080/","height":153}},"source":["%%time\n","test_data = pd.read_csv(\"/content/gdrive/My Drive/Colab Notebooks/ValueLabs/Results.csv\")\n","print(pd.DataFrame(test_data.head(1)).columns)\n","result = predict_distractor(test_data.loc[[1]])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Index(['question', 'answer_text', 'distractor'], dtype='object')\n","['in the summer high season  finland does nt seem to sleep because'\n"," 'the sun is out at night' 'makes us more clever']\n","fuzzy features..\n","(99388, 45)\n","(99388, 345)\n","CPU times: user 1h 31min 44s, sys: 2min, total: 1h 33min 44s\n","Wall time: 1h 33min 50s\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"RHCZV6o7xdvT","colab_type":"code","outputId":"a8b98c32-74ff-4f23-dbbd-f065971e958f","executionInfo":{"status":"ok","timestamp":1573464522514,"user_tz":-330,"elapsed":1668,"user":{"displayName":"Bhanu sree Balisetty","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDP21Fovgv-54GdUi8eB52GjzNw-SFegRrRjvMChg=s64","userId":"06056630683889706511"}},"colab":{"base_uri":"https://localhost:8080/","height":204}},"source":["print(\"Question\")\n","print(result.iloc[0].question)\n","print(\"Answer\")\n","print(result.iloc[0].answer_text)\n","print(\"Distractors\")\n","result.iloc[200:205].distractor"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Question\n","in the summer high season  finland does nt seem to sleep because\n","Answer\n","the sun is out at night\n","Distractors\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["1226    contributed to the invention of atomic bomb\n","1228     true happiness comes from spiritual riches\n","1230         the person who called the firefighters\n","1241      different people have different attitudes\n","1247                               the walk a thons\n","Name: distractor, dtype: object"]},"metadata":{"tags":[]},"execution_count":72}]},{"cell_type":"markdown","metadata":{"id":"g9lyTPa5gLNN","colab_type":"text"},"source":["# Extensions\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"lSb5P-7vfLTi","colab":{}},"source":["def get_top_n_similiarities(vec1, vec2):\n","    n=3\n","    distances = []\n","    for index, i in enumerate(v2):\n","        vec2 = train_data_vec[index]\n","        d = np.dot(vec1, vec2)/(np.linalg.norm(vec1)*np.linalg.norm(vec2))\n","        if len(distances) < n:\n","            distances.append((i,d))\n","        else:\n","            distances.sort(key=lambda x: x[1], reverse=True)\n","            if (distances[-1][1]<d):\n","                distances.pop()\n","                distances.append((i,d))\n","    result = ''\n","    for index, i in enumerate(distances):\n","        sen = ' '.join(i[0])\n","        result = result + \"'\"+sen+\"'\"\n","        if index!=len(distances)-1:\n","            result+=\",\"\n","    return result\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZPWNaMQpcn2k","colab_type":"text"},"source":["#References\n","1. https://stackoverflow.com/questions/16096627/selecting-a-row-of-pandas-series-dataframe-by-integer-index"]}]}